Parking Management System – Architecture and Implementation Plan
Overview
We are designing a parking management system for a residential building that allows residents to share and reserve parking spots with a real-time availability view. The system will have a modern microservice-inspired architecture with a decoupled frontend and backend, emphasizing scalability, maintainability, and best practices. Key features include real-time parking spot updates via WebSockets, user-contributed parking listings, advance reservation scheduling, a credit-based incentive system, and flexible data querying via GraphQL. To support these features, we will use a robust tech stack: FastAPI for the backend (with Python), PostgreSQL as the primary database, React + TailwindCSS for a responsive frontend (with Dark Mode support), Redis for session management and caching, and Docker + Kubernetes for containerization and deployment. Continuous integration and deployment (CI/CD) will be set up with GitHub Actions, deploying our application to an Azure VM environment for testing and production. 

Example of a modern web application architecture with a React frontend consuming a FastAPI backend (REST API) connected to a PostgreSQL database. Our system will follow a similar architecture, augmented with WebSockets for real-time updates and Redis for caching and session management.
Tech Stack and System Architecture
Backend: FastAPI (Python) – chosen for its high performance and intuitive async support, to serve a RESTful API and handle WebSocket connections. FastAPI will interact with PostgreSQL for persistent data storage. We will design database schemas with proper indexes (e.g., on parking spot availability and reservation times) to ensure query performance for finding available spots quickly.
Frontend: React (JavaScript/TypeScript) – a single-page application for users to view available parking spots, add their spots, and make reservations. We will use TailwindCSS for styling, enabling rapid UI development and an easy Dark Mode toggle. The frontend communicates with the FastAPI backend via REST (and subscribes to WebSocket updates for real-time changes).
Real-Time Communication: WebSockets – to push immediate updates of parking availability to all connected clients. FastAPI supports WebSockets out-of-the-box, allowing the server to notify clients when a spot becomes free or gets reserved without polling. This ensures a live-updating parking map for users.
Flexible Queries: GraphQL – in addition to REST endpoints, we will expose a GraphQL API for clients that need more tailored queries. FastAPI can integrate with GraphQL libraries (like Strawberry or Ariadne) to serve a /graphql endpoint​
FASTAPI.TIANGOLO.COM
. This gives clients the option to fetch exactly the data they need (for example, a single query to get all free spots with their owner’s name and next reservation time).
Caching & Session Store: Redis – a fast in-memory data store used to manage user sessions and cache frequently accessed data. Storing session data (or token blacklists, etc.) in Redis ensures horizontal scalability (all instances share session state) and improves performance for read-heavy operations​
RESTACK.IO
. We can also use Redis pub/sub as needed to coordinate real-time messages across multiple backend instances.
Authentication: JWT or OAuth2 – we will secure the system by requiring users to authenticate (e.g., login with password) and obtain a JWT for subsequent requests. FastAPI provides OAuth2 with JWT support out-of-the-box​
DEVCURRENT.COM
. We will decide between simple JWT auth (sufficient for internal users and stateless sessions) or an OAuth2 integration (if we plan to allow third-party logins or more complex token management). Either way, password hashing and role-based access will be implemented following best practices (e.g., using OAuth2PasswordBearer for JWT verification and scopes).
Containerization & Orchestration: Docker and Kubernetes – all components (backend, frontend, perhaps even the database in development) will be containerized with Docker for consistency across environments. Kubernetes will manage these containers in production, providing easy scaling, load balancing, and self-healing deployments. We’ll use Kubernetes Deployments for our FastAPI app and React frontend, Services for internal communication (and possibly an Ingress for external access), ensuring an efficient deployment and the ability to scale out as usage grows.
CI/CD Pipeline: GitHub Actions – for automated testing, building, and deployment. Every change pushed to the repository will trigger workflows that run unit tests, build Docker images, and deploy the updated application to an Azure VM (which will host our Kubernetes cluster or Docker containers). This guarantees that new code is integrated continuously and deployed in a repeatable, stable way.
Overall, this architecture separates concerns cleanly: the frontend focuses on user experience, the backend handles business logic and data, and infrastructure components (Redis, Docker/K8s, CI/CD) ensure reliability and scalability. Next, we detail how each major feature will be implemented following best practices.
Real-Time Parking Availability (WebSockets)
To provide live updates of available parking spots, we will implement WebSocket communication in the FastAPI backend. Clients (the React app) can open a WebSocket connection to the server (e.g., to an endpoint like /ws/updates) and subscribe to parking availability events. Whenever a user marks their spot as available or a reservation is made/ended, the backend will broadcast an update to all connected clients. FastAPI makes handling WebSockets straightforward by allowing endpoint definitions that accept a WebSocket object – we will maintain a list of active connections and use a broadcast mechanism to send messages to all clients when the parking data changes. One important consideration is ensuring this works in a distributed setting. The basic FastAPI example uses an in-memory list of connections, which works for a single process but would not automatically sync across multiple instances​
FASTAPI.TIANGOLO.COM
. In a Kubernetes deployment with multiple replicas of the backend, we’ll incorporate a solution for inter-instance messaging. A common approach is to use Redis Pub/Sub or a library like Broadcaster to propagate WebSocket messages to all server instances​
FASTAPI.TIANGOLO.COM
. By publishing events to a Redis channel, all FastAPI instances can subscribe and forward the message to their clients, ensuring that real-time updates are truly real-time for everyone connected, even in a scaled-out environment. From a frontend perspective, upon connecting to the WebSocket, the client will receive events such as “spot X is now free” or “spot Y has been reserved until 5 PM”. The React app will update the UI accordingly (e.g., highlight the spot as taken or free). This is far more efficient than polling the server every few seconds and provides a snappy user experience. To implement this robustly, we will:
Define a connection manager in FastAPI to track active WebSocket connections (using a list or set of websockets).
When a user changes a spot’s status or creates a reservation, call the manager to broadcast the update (could be a simple text or JSON message containing the spot ID and new status).
If running multiple backend pods, use Redis to publish the event; each pod’s WebSocket handler subscribes and rebroadcasts to its own clients.
Secure the WebSocket endpoint (only authenticated users should subscribe) – we can require a JWT token as part of the connection query params or perform an authentication check on connect.
Handle disconnects gracefully and remove connections to avoid memory leaks.
This setup will ensure that the “Available Spots” view in the application is always up-to-date in real time, which is crucial for a smooth user experience (nobody wants to try reserving a spot that just got taken a second ago).
User-Added Parking Spots & Availability Management
A core feature is allowing users to contribute their own parking spots into the system. Each resident can register the details of their parking spot (e.g., spot number, location, etc.) via the frontend. The backend will provide an API endpoint (e.g., POST /spots) to add a new spot to the database. We will associate each parking spot record with the user who owns it (a foreign key relationship to a Users table). Users can also toggle the availability of their spot. For example, if a user knows their spot will be empty for the next 2 hours, they can mark it as “available” for others to use. This action (perhaps through a button in the UI) will send a request to the backend (e.g., PUT /spots/{id}/availability) to update the spot’s status. We’ll enforce that only the owner of a spot (or an admin) can change its availability status. On the backend, this will update a field (like is_available) in the database for that spot, and trigger a WebSocket event to notify all clients of the change in real time. Data Model: We’ll have a table ParkingSpot with fields such as id, owner_id (references Users), location or spot_number, is_available (boolean), and perhaps current_reservation_id if we want to link it to an active reservation. By indexing the is_available field, queries for available spots (which will be frequent) become faster. We might also maintain a history or usage count for analytics, but that’s optional. On the frontend, the user will have a dashboard listing their spots and controls to mark them free or occupied. When marked free, other users should immediately see it in the available list (thanks to WebSockets). Conversely, when the owner marks it occupied again (perhaps they returned early), it should disappear from others’ available list. We will carefully ensure that toggling availability doesn’t conflict with existing reservations: e.g., if a spot is reserved for later, the owner shouldn’t mark it unavailable during that reserved period without canceling the reservation. The backend will include validation to prevent such conflicts (for instance, if a reservation exists for now or the near future, you may not override availability). To keep code maintainable, the logic for adding spots and toggling availability will be encapsulated in the backend as separate router endpoints and service functions. For example, we might have a spots.py router file in FastAPI with routes: create_spot, set_spot_availability, list_spots (with filters), etc. The business logic (like “if reservation exists, deny making unavailable”) goes into a service layer or the route function with clear checks, so any future changes (like new rules or notifications) can be made in one place without breaking other parts.
Reservation System (Scheduled Booking)
The system allows users to reserve a parking spot for a defined time window. This means a user can pick a specific parking spot and a time range (start and end times, or perhaps a start time with a fixed duration) to book it in advance. Implementing this requires both front and backend components: a UI for selecting time slots, and backend logic to handle booking and avoid double-booking. On the backend, we will introduce a Reservation model/table. Key fields for a reservation include: id, spot_id (the parking spot being reserved), user_id (the user who made the reservation), start_time, end_time. When a user attempts to reserve a spot, the backend will:
Validate that the spot is actually available for the requested period. This likely involves querying the reservations table for any overlapping reservation on the same spot. We can enforce at the database level by ensuring no two reservations overlap on the same spot (for example, using a unique index on spot_id + time slots might be complex, so we’ll mainly rely on application logic with proper locking or transaction isolation).
If available, create a new reservation entry and possibly set the spot’s status to “reserved” if the reservation is for the current time or immediate future. If the reservation is for a future time, we might keep the spot available until that time, but we’ll block other reservations overlapping it.
Optionally, we could implement a scheduler or background job to automatically mark the spot as occupied at the reservation start time and free it at the end time. However, given the complexity, a simpler approach is to rely on the users (the person who reserved or the spot owner) to mark when the spot is actually taken or freed. In future, integrating something like Celery could help automate this (e.g., release a spot at end_time if not done manually).
When a reservation is created, we will send a WebSocket update to notify others (so the spot might appear as “reserved” or unavailable for that period on the UI). We will also likely send a confirmation back to the reserving user (HTTP response). The system should also allow users to cancel their reservation (before it starts, within a reasonable time window) to free up the spot for others and possibly avoid losing credits if we implement a penalty. Frontend: Users will see which spots are available now and in the future. We might represent near-future availability by a different color or indicator (if a spot is free now but has a reservation in an hour, the UI could reflect that). The reservation form will let a user select a spot and pick a start/end time (probably with some constraints like maximum duration or only one reservation at a time per user, depending on building policy). Upon successful booking, the UI will show the reserved status. We’ll ensure the business rules are enforced consistently: for instance, one user shouldn’t double-book multiple spots for the same time (we might allow only one active reservation per user at a time). Also, a user cannot reserve their own spot (since presumably they already have it by default). Such validations will be coded in the backend route handling the reservation creation. Transactions will be used when necessary – for example, if reserving a spot involves adjusting credits (see next section), we will perform those operations together so that we don’t end up deducting credits without a reservation or vice versa.
Credit Management System
To encourage sharing, the system uses a credit economy. Each user starts with 50 credits. Whenever someone uses another person’s parking spot, a certain number of credits (3 credits as specified) are transferred from the user who parked to the owner of the spot. This incentivizes owners to make their spots available (they earn credits) and requires users to spend credits to use others’ spots, creating a fair exchange. Implementation: We will maintain a credits balance for each user (could simply be a column in the Users table). Initially, when a user registers, we set their credits to 50. Each time a reservation is completed (or perhaps when it is made), we will deduct 3 credits from the reserving user and add 3 to the spot owner’s credits. It might be wise to only transfer credits once the reservation is actually used or completed to avoid scenarios like paying for a reservation that gets canceled. However, an easier approach is to deduct at booking time to ensure commitment, and if canceled, maybe refund the credits. The exact policy can be defined by the product requirements. In code, handling a credit transfer is straightforward but must be atomic. We will likely do this in a database transaction:
Check that the reserving user has enough credits (>= 3) before allowing the reservation. If not, return an error (they need sufficient credits).
If yes, subtract 3 from their balance and add 3 to the owner’s balance, and save the reservation. All these operations happen in one transaction so that we don’t end up in a partial state. Using an ORM like SQLAlchemy, we can lock the rows or use SELECT ... FOR UPDATE to prevent race conditions if two operations happen concurrently on the same user’s credits.
Possibly log the transaction (we could have a CreditsTransaction table for history, but that might be overkill initially).
We will expose each user’s credit balance via the API (e.g., when they fetch their profile or on login). The frontend can display the current credits and update it after each reservation. Also, the UI should inform the user of the cost when they make a reservation (“This will cost 3 credits”) and perhaps show the owner of the spot who will receive those credits. Additionally, if a user’s credits drop to zero or negative, we might restrict them from making further reservations until they earn or purchase more credits. In the initial version, credits circulate within the community (one’s loss is another’s gain), so the total credits in the system remain constant. We should also handle cases like if the owner of a spot is not using the system anymore – their credits accumulate but that’s fine as it doesn’t break anything. By tracking and updating credits carefully, we ensure that the incentive mechanism is fair and cannot be easily gamed. We will include checks to prevent a user from somehow transferring credits to themselves or other loopholes (for instance, one user and their friend repeatedly reserving each other’s spots just to transfer credits back and forth – we might need rate limits or rules if that becomes a concern).
GraphQL Support for Flexible Queries
While our REST API will cover all the standard operations (CRUD for spots, making reservations, etc.), we plan to include a GraphQL endpoint to give clients more flexibility in how they query the data. GraphQL can be useful if, for example, the building decides to develop a more complex front-end dashboard or integrate with third-party services that benefit from querying multiple pieces of data in one request. We will integrate GraphQL into FastAPI using a library that is known to work well with it. The FastAPI documentation recommends Strawberry as a GraphQL library that aligns with FastAPI’s design (leveraging Python type hints)​
FASTAPI.TIANGOLO.COM
. Strawberry allows us to define schema types and query/mutation functions in Python, and then mount a GraphQL endpoint onto our FastAPI app. Concretely, we’ll do something like: define GraphQL types for User, ParkingSpot, Reservation and so on, then define Query fields (for example, a query to list spots could accept a filter for availability or owner, etc.). We’ll include the GraphQL route at, say, /graphql. This will run in parallel with our REST routes – FastAPI can handle both simultaneously. Having GraphQL means a client could ask, for instance: “Give me all available spots with their owner’s name and the next reservation time” in one query, rather than calling multiple REST endpoints. It also means if the frontend needs less data, it can request just that (to reduce payload). One thing to note: implementing GraphQL is optional for our use case since REST covers most needs, but since it’s a requirement, we’ll include it carefully. We must ensure our GraphQL endpoints are secured (we’ll need to integrate our authentication, perhaps by requiring a token in the headers for GraphQL requests as well). Thankfully, since GraphQL runs in the same app, we can use the same dependency injection for auth. By supporting GraphQL, we future-proof the API for more complex client interactions and give developers using our system a choice of API style. We will document the GraphQL schema for any consumers. If GraphQL adds significant complexity, we will keep it as an optional component that can be enabled as needed (so it doesn’t interfere with core functionality if not used).
Session Management and Caching with Redis
In a distributed web application, managing user sessions and caching frequently accessed data is essential for both performance and a smooth user experience. We will incorporate Redis to handle these aspects:
Session Management: If we choose to implement stateful sessions (for example, user logs in and we store a session token or data on the server side), Redis is an ideal storage for that. Even though JWTs are stateless, we might want to store some session info like a refresh token or to invalidate tokens (JWT blacklisting on logout). By keeping this in Redis, all instances of our FastAPI app (in Kubernetes) can access the same session data. This avoids the issue of a user potentially hitting a different server that doesn’t know their session. FastAPI can integrate a middleware for server-side sessions; otherwise, we simply manage tokens in Redis manually (set token -> user mapping with expiry).
Caching: Redis will be used to cache results of expensive queries or commonly requested data. For instance, the list of currently available spots could be cached in Redis with a short TTL. Instead of querying PostgreSQL on every page refresh or every few seconds, the backend can serve the data from Redis, updating the cache whenever a spot availability changes. This drastically improves performance and reduces load on the database​
RESTACK.IO
, especially during peak hours when many users are checking for spots. Similarly, we might cache user profiles or settings, or the list of upcoming reservations, etc., as appropriate.
Real-time data propagation: As mentioned in the WebSockets section, Redis can also act as a pub/sub broker for real-time messages. FastAPI instances can publish events (like “spot 5 reserved”) to a Redis channel, and all instances can subscribe to that channel to get the event and notify their WebSocket clients. This is a lightweight way to broadcast across processes and is much simpler than building a custom messaging system.
We will connect to Redis using an async client (such as aioredis or Redis-py with asyncio support) and establish this connection at startup of the FastAPI app (FastAPI allows defining startup events to initialize resources like DB or Redis connections). On shutdown, we’ll close the Redis connection gracefully. From an implementation standpoint, using Redis will involve writing a few utility functions: e.g., get_from_cache(key), set_cache(key, value, ttl), etc., and similarly for session management (like store_session(token, user_id) and get_session(token)). We will keep these in a separate module (perhaps utils/cache.py or services/redis_client.py) so that cache logic is centralized. This way, if our caching strategy changes, we only update it in one place. In summary, Redis acts as a high-speed adjunct to our database and application, ensuring that our system remains responsive and scalable under load by reducing direct database hits and by enabling shared state management. This aligns with our goal of high performance and a smooth user experience even as the system scales​
RESTACK.IO
.
Containerization with Docker and Deployment on Kubernetes
To ensure consistency from development to production and to allow scalable deployment, we will containerize the application using Docker. Each major component will have its own Docker image: for example, one for the FastAPI backend, one for the React frontend (which could be served via a simple Node/NGINX server), and possibly separate images for auxiliary services (though PostgreSQL and Redis we might use official images rather than build our own). Dockerizing the app means packaging the code along with all dependencies into an image, so it runs the same everywhere. Our FastAPI Dockerfile will start from a Python base image, install requirements, copy the application code, and then launch the app using Uvicorn (or Gunicorn with Uvicorn workers) for production. The React frontend Dockerfile would likely use a Node image to build the static files, then an Nginx image to serve the compiled static site. We will follow Docker best practices (small image sizes, multi-stage builds for the frontend, not running as root if possible, etc.). For deployment, we’ll use Kubernetes to manage the containers. Kubernetes will allow us to run multiple instances (pods) of the backend to handle more users, and can likewise manage the frontend container and a Redis container. In our setup, since we are using an Azure VM, we have a couple of options: we could set up a single-node Kubernetes cluster on that VM (using something like MicroK8s or Minikube in VM, or even Azure Kubernetes Service if we extend beyond a single VM), or use Docker Compose on the VM for simplicity. However, the requirement explicitly mentions Kubernetes, so we will assume we’ll configure a Kubernetes cluster (even if single-node) on the Azure VM. Kubernetes Deployment Plan:
Create a Kubernetes Deployment for the FastAPI app. This will reference the Docker image built for the backend. We’ll configure environment variables for things like database URL, Redis URL, secret keys (these can be provided via Kubernetes Secrets/ConfigMaps). We will set the number of replicas (initially maybe 1-2, but can scale out). We’ll also add readiness and liveness probes for the container – e.g., an HTTP health-check endpoint in FastAPI that the probe can call to ensure the app is up.
Create a Service for the FastAPI deployment to expose it internally (and potentially externally). If we want the frontend to call the API, and both run in the cluster, an internal Service with DNS (e.g., http://fastapi-service) is handy. But for external access (if the frontend is separate or for testing API directly), we might set up an Ingress resource that routes traffic on a certain path or subdomain to the FastAPI service. Using an Ingress controller will also simplify using TLS (we can later add HTTPS with a certificate).
Create a Deployment for the React frontend. Alternatively, since the frontend is static, we could even serve it from the FastAPI or host it on Azure Storage/Static web app. But to keep things contained, an Nginx serving the static files in a container is fine. We’ll expose it via a Service and maybe the same Ingress (so that the domain root goes to the frontend, and /api or /graphql goes to backend).
For PostgreSQL, in a production scenario we might use a managed database service or run a single instance on the VM (but running DB in Kubernetes can be done with a StatefulSet). Since it’s a bit advanced to manage DB clustering, for now we can deploy a single PostgreSQL pod with a persistent volume claim on the VM. In Azure, another approach is to use Azure Database for PostgreSQL and just provide the connection string to our app (that might be more robust). Similarly for Redis, we can either run a Redis pod (with no persistent storage needed, just memory) or use Azure Cache for Redis. Initially, a Redis Deployment with one replica is fine (we saw an example config in best practices guides​
RESTACK.IO
​
RESTACK.IO
).
Once Kubernetes manifests (YAML files) for all these are written and tested, deployment is as easy as applying them. Kubernetes will take care of starting containers, restarting on failure, etc.
Using Kubernetes gives us not only scaling, but also self-healing (if a container crashes, it’s restarted), and the ability to do rolling updates (deploy new version with zero downtime). For example, when our CI/CD pipeline pushes a new image, we can trigger a rolling update in Kubernetes which will gradually replace pods with new ones. In summary, Docker and Kubernetes will ensure our deployment process is smooth and our application can handle growth. This approach is considered an industry best practice for cloud applications, providing a consistent environment and enabling efficient resource usage. We will source control our Dockerfiles and Kubernetes deployment files, so the infrastructure-as-code is also maintained in the repository (making any environment changes traceable and reviewable).
CI/CD Pipeline with GitHub Actions and Deployment to Azure
We will set up a Continuous Integration/Continuous Deployment (CI/CD) pipeline using GitHub Actions to automate testing and deployment. This brings a professional workflow to the project, ensuring that every code change is verified and deployed in a consistent manner:
Continuous Integration (CI): On each push (or pull request) to the repository, GitHub Actions will run a workflow that executes our test suite. We will write unit and integration tests for the backend (and possibly some for the frontend). This step is crucial to catch any breaking changes early. For example, if a change in one module unintentionally breaks another part of the system, the tests will fail and alert us before merging. This addresses the requirement of making sure “any change in one file doesn’t break the system” – by having good test coverage and CI, we gain confidence in our changes. Additionally, we can run linting (checking code style) and type checks during CI to maintain code quality.
Continuous Deployment (CD): After tests pass, the pipeline can proceed to build Docker images for the updated application. We’ll have GitHub Actions build the FastAPI image and the React image (perhaps in parallel jobs). Once built, these images will be pushed to a container registry (it could be Docker Hub or Azure Container Registry). After pushing the new image, the next step is deploying it to Azure. Since we have an Azure VM, one way is to use SSH or Azure CLI actions. For instance, the workflow can SSH into the VM and run kubectl apply -f deployment.yaml to update the Kubernetes deployment with the new image (assuming the VM is our K8s master or part of cluster). Alternatively, we might run a lightweight agent on the VM that listens for webhook or use Azure DevOps pipelines, but to keep it simple, an SSH approach is fine.
We will configure secrets in GitHub (like the Azure VM’s IP, SSH key, or any credentials needed for the registry or VM) so that the pipeline can securely connect and deploy. GitHub Actions allows storing these as encrypted secrets.
Environment Configuration: As part of CI/CD, we might have multiple environments. For example, a staging environment (perhaps another VM or a separate namespace in K8s) where we deploy on each commit to test in a production-like setting, and a production environment where we deploy maybe on a tagged release or after manual approval. This adds professionalism by not just pushing every commit to prod without verification. However, given the scope, we might stick to a single environment but still follow the practice of versioning.
Azure VM Setup: On the Azure VM side, we need to have Docker and Kubernetes installed (if it’s single-node k8s). We’ll also ensure the VM has access to the container registry (if private). The CI can handle logging in to the registry before push/pull. If not using Kubernetes, an alternative is using Docker Compose on the VM and the CI could just replace containers by pulling new images. But since K8s was specified, we’ll go that route.
By having this automated pipeline, deployment becomes repeatable and less error-prone. We won’t need to manually FTP files or manually run docker builds on the server – everything is scriptable and runs on every change. This not only saves time but also ensures that the system state is always consistent with the repository (no “worked on my machine” issues). Additionally, with GitHub Actions we can incorporate other best practices: for instance, running a security scan of dependencies (to catch vulnerable libraries), or enforcing code review before merge. These are things that keep the project maintainable in the long run. To summarize, the CI/CD setup will look like: developer pushes code -> GitHub Actions triggers -> run tests -> build Docker images -> deploy to Azure. After that, the updated system is live. We’ll document the pipeline and perhaps include a status badge in the repo README for visibility. This way, our deployment process is as modern and robust as the application itself.
Secure Authentication & Authorization (JWT/OAuth2)
Security is paramount since we’re dealing with personal data (user accounts, possibly contact info, and controlling access to physical parking spots). We will implement a secure authentication and authorization system using industry best practices. Authentication: We’ll use token-based auth. FastAPI’s built-in OAuth2 support with password flow will let us implement login and token issuance easily. Essentially, when a user logs in (provides username/email and password), the backend will verify the credentials (after hashing the provided password and comparing to the stored hash), and if valid, return a JWT (JSON Web Token) that is signed with our secret key. This JWT will include the user’s identity and possibly their roles/permissions in its payload. FastAPI provides the OAuth2PasswordBearer and OAuth2PasswordRequestForm utilities to facilitate this. The result is a Bearer token that the frontend will store (probably in memory or a secure place) and send with each request to protected endpoints. We favor JWT because it is stateless and scalable – the server doesn’t need to keep session information for basic auth checks, the token itself carries the info and just needs verification​
DEVCURRENT.COM
. That said, we must protect the secret key and use strong signing algorithms (HS256 or RS256, etc.). We’ll also set tokens to expire (maybe after e.g. 24 hours) to mitigate risk if one is stolen. For longer sessions, we could implement refresh tokens. Authorization: With JWT in place, each request the user makes to, say, reserve a spot or add a parking spot will include the token. FastAPI will decode and verify the token in a dependency (get_current_user) and provide the current user object to the route handler. We will use this to enforce that, for example, only the authenticated user can add/modify their own spots, only logged-in users can make reservations, and possibly an admin role could be required for certain endpoints (like viewing all usage stats, etc.). We can include a “role” claim in JWT or maintain roles in the database and check accordingly. If we decide OAuth2 with an external provider is needed (for instance, allowing login with Google or Microsoft since this is for building residents who might prefer using an existing account), we could integrate an OAuth2 authorization code flow. This is more complex, so unless specifically needed, JWT with our own username/password system is sufficient. The mention of “JWT or OAuth2 depending on what fits” suggests we evaluate which is appropriate – JWT (with OAuth2 password flow) fits a custom auth system, whereas OAuth2 with external providers fits if we expect users to use corporate logins or such. Given this is a closed system (building residents), we will likely manage our own user database and use JWTs for simplicity. To further secure the system, we will:
Use HTTPS in production so that tokens (and all data) are encrypted in transit.
Store password hashes in the database (never plaintext) using a strong hashing algorithm (FastAPI’s docs use Passlib with bcrypt which we will follow).
Possibly implement rate limiting on auth endpoints to prevent brute force (there are libraries like fastapi-limiter using Redis​
APP-GENERATOR.DEV
 if needed).
Ensure sensitive endpoints (like transferring credits, making reservations) double-check authorization – the combination of JWT auth and backend checks (like ensuring you can’t reserve someone else’s spot as your own, etc.) provides defense in depth.
Log security events (like failed logins, or access denied) for audit purposes.
By doing this, we create a secure authentication system that users can trust. They will log in once and then operate under that identity reliably across the system. If any user’s token is compromised or if we want to force logout, we can maintain a token revocation list in Redis (mark certain token IDs as invalid). This adds a bit of complexity but is doable. In many cases for a simpler system, one might not implement logout invalidation (just rely on short token expiry), which might be acceptable here. In summary, our approach is to use tried-and-true methods for auth: OAuth2 with JWT, which FastAPI supports, giving us a solid security foundation with minimal overhead​
DEVCURRENT.COM
. This ensures only authorized users can use the system’s features and ties all actions (reservations, spot management, etc.) to a verified user identity.
Project Structure and Best Practices
To meet the goal of a maintainable, professional codebase, we will organize the project files and directories following best practices for FastAPI and general software architecture. A clear structure helps developers navigate the code and ensures that making changes in one part of the system is less likely to unintentionally break another part. Backend Project Structure: We will use a modular layout for the FastAPI app, for example:
bash
Copy
Edit
backend/
├── app/
│   ├── main.py          # Creates FastAPI app, includes routers, startup events
│   ├── models/          # SQLAlchemy models (User, ParkingSpot, Reservation, etc.)
│   ├── schemas/         # Pydantic models (request/response schemas)
│   ├── routers/         # API route definitions, e.g., auth.py, spots.py, reservations.py
│   ├── services/        # Business logic and database operations (could also call this 'crud' or 'usecases')
│   ├── core/            # Core modules (config, security, utils)
│   │   ├── config.py    # Settings (database URL, secret key, etc.) possibly using pydantic BaseSettings
│   │   ├── security.py  # Auth-related helpers (JWT encoding/decoding, password hashing)
│   │   └── ... 
│   └── db.py            # Database session setup (SQLAlchemy session local and engine)
├── tests/               # Test cases for backend
├── Dockerfile           # Docker configuration for backend
└── requirements.txt     # Python dependencies
This structure separates concerns: for example, the routers will contain just the API endpoint functions and rely on services for heavy lifting. Each router can include the FastAPI Depends for auth to protect routes. The models define the DB schema and relationships, while schemas define what data is expected in and out (these also help with automatic validation thanks to Pydantic). By keeping these in different modules, if the database layer changes (say we switch ORM or tweak a model), the impact on the API layer is minimized (only change how the service fetches data, the API interface can remain the same). This reduces the chance that a change “breaks the system” as long as the contracts between layers stay consistent. It also makes unit testing easier (we can test service logic without running the actual API, by mocking the DB, etc.). We will also follow best practices like:
Using environment variables for configuration (with a config module to read them), so sensitive info and environment-specific settings aren’t hardcoded.
Writing docstrings and comments where appropriate to document the purpose of modules and classes.
Applying linting and formatting (like flake8 and black) to keep code style uniform. This can be part of the CI checks.
For the frontend, the structure might be something like a standard React app (created by Create React App or Vite): with components organized logically, perhaps a context for auth state, etc. We will ensure the build outputs to a build/ folder that our Dockerfile can serve. Database Migrations: To avoid manual DB schema changes, we’ll use a migration tool (likely Alembic with SQLAlchemy) to manage database schema versions. This means whenever we alter or add a model, we create a migration script to apply changes. This is professional practice to keep the production database schema in sync with the models code. The migration scripts will be stored in a folder (say backend/migrations/) and can be run as part of deployment. Testing: We will write tests for critical parts of the system: e.g., a test for “cannot double book a spot”, a test for “credit transfers correctly on reservation”, tests for auth (correctly rejects invalid token, etc.), and so on. This ensures our assumptions hold and helps catch regressions. We might use FastAPI’s TestClient for API endpoint testing and pytest for general testing. Documentation: Thanks to FastAPI, we get automatic API docs (Swagger UI) for REST endpoints. We’ll also maintain a README or Wiki for how to deploy and how to run the system. Good developer documentation is part of best practices. Finally, to address “each change in one file doesn’t break the whole system”: Our strategy is multi-fold – a clean architecture as described, comprehensive tests, and CI enforcement. If a change does break something (e.g., changing a function signature that other modules use), our CI tests should flag it. We’ll perform code reviews for significant changes to have an extra pair of eyes. The use of type hints can help catch some errors as well (mypy static analysis could be added). All these practices combined mean we can evolve the system with confidence. By diligently following these best practices, the project will remain robust, clean, and professional. New team members should find it easy to understand, and future feature additions or modifications can be done with minimal friction.
Conclusion
🚀 Next Steps: With this plan in place, we will proceed to implement the system component by component. We’ll start by setting up the FastAPI project structure and database models, then implement real-time updates and core features like reservations and credits, followed by the React frontend integration. Simultaneously, we’ll configure our development environment with Docker and write GitHub Actions workflows for CI/CD. This parking management system is designed with scalability (through Kubernetes and async processing), flexibility (offering both REST and GraphQL APIs), and reliability (thanks to Redis caching and a sound testing strategy) in mind. By using modern frameworks and tools, and adhering to best practices, we ensure that the end product will be maintainable and ready for future expansion. We will provide updates as we reach milestones in the implementation. With a solid architecture blueprint and workflow established, the development can now move forward confidently towards a successful, professional-grade system.